\section{Unified interpolation with kernels and polynomials}
\label{sec:lag_uni}

We will now present a unified framework for kernel and polynomial approximation. Consider the hybrid \emph{interpolant} given by:
\begin{align}
s(\vx) = \sum\limits_{k=1}^N c_k \phi\lf(\ep \|\vx-\vx_k\|\rt) + \sum\limits_{j=1}^{\ell + d \choose d} d_j p_j(\vx),
\label{eq:hybrid}
\end{align}
where $\phi$ is an RBF and $\ep$ its shape parameter, and the $p_j$ functions form a basis for the space of total degree polynomials of degree $\ell$ in $d$ dimensions.  As mentioned previously, while it is traditional to use conditionally positive-definite kernels with global support as choices for $\phi$, we now assume $\phi$ is a \emph{compactly-supported} and \emph{positive-definite} RBF.  While there are many such RBFs~\cite{Wendland,Wu,Buhmann,Euclid}, we focus on the Wendland functions given by:
\begin{align}
\phi_{m,n}(r) = \begin{cases}
\frac{1}{\Gamma(n) 2^{n-1}} \int\limits_{r}^1 s(1-s)^m (s^2 - r^2)^{n-1} ds & \text{for } 0 \leq r \leq 1, \\ 
0 & \text{for } r > 1.
\end{cases}
\end{align}
For the Wendland functions, the shape parameter $\ep$ serves as the reciprocal of the radius of support $r$. Unfortunately, these RBFs have historically traded off increased sparsity (and improved conditioning) in the interpolation matrix for decreased accuracy and convergence rates~\cite{Fasshauer:2007}. Recent work has shown~\cite{FlyerNS,FlyerPHS,FlyerElliptic,SFJCP2018,SWJCP2018} that this decrease in accuracy can be overcome by augmenting the RBF interpolant with polynomials. However, thus far, to the best of our knowledge, the only RBFs used have been those with global support, such as the positive-definite Gaussian RBF and the conditionally positive-definite polyharmonic splines. Our goal in this work is to both demonstrate that this technique works in the context of Wendland RBFs, and to demonstrate that the use of compactly-supported RBFs creates a highly general and flexible unified framework for kernel and polynomial approximation. 

To that end, we shall now present and discuss several special cases of the above interpolation scheme. We will also discuss efficient solution of the resulting linear systems for each of the limiting cases. In the following discussion, let $X = \{\vx_k \}_{k=1}^N$ be the set of data sites for interpolation. Further, let $q$ be the smallest pairwise distance between of nodes in $X$, and $w$ be the largest pairwise distance. Finally, let $r = 1/\ep$ be the support of the Wendland RBF.

\subsection{The polynomial limit ($r < q$)}

\subsubsection{Interpolation}
When the support $r$ of any Wendland RBF is made smaller than $q$, the Wendland RBFs each take on the value of $1$ at the nodes and $0$ elsewhere, with transitions between the nodes dictated by the smoothness of the Wendland RBF. Thus, the interpolation constraints \eqref{eq:interp_constraints} lead to the following linear system:
\begin{align}
\begin{bmatrix}
I & P \\
P^T & 0
\end{bmatrix}
\begin{bmatrix}
\vc\\
\vd
\end{bmatrix}
=
\begin{bmatrix}
\vf\\
{\bf 0}
\end{bmatrix},
\label{eq:small_limit_linsys}
\end{align}
where $I$ is the $N \times N$ identity matrix and $P$ is the Vandermonde-like matrix of evaluations of the polynomial basis at the interpolation nodes. To better understand this linear system, we use block Gaussian elimination to rewrite this as two equations:
\begin{align}
P^T P \vd &= P^T \vf,\\
\vc &= \vf - P\vd.
\end{align}
The first system of equations is immediately recognizable as the system of normal equations arising from the least squares problem of minimizing $\|P \vd - \vf\|_2^2$. Thus, in this context, despite the presence of RBFs in the approximation, the polynomial coefficients $\vd$ are identical to the coefficients obtained from the least-squares problem. Clearly, the RBF coefficients $\vc$ only serve to enforce interpolation at the collocation nodes. 

\subsubsection{Linear Algebra}

In the polynomial limit, the unified interpolant \eqref{eq:hybrid} can be computed efficiently using the QR decomposition as follows:
\begin{enumerate}
\item Compute $P = QR$, the (reduced) QR decomposition of the polynomial least-squares matrix $P$. 
\item Solve the Schur complement system for the polynomial coefficients $\vd$ as $\vd = R^{-1} Q^T \vf$. 
\item Compute the RBF coefficients $\vc = \vf - P \vd$.
\end{enumerate}
The SVD could also be used to find $\vd$. When solved in this fashion, the block system \eqref{eq:small_limit_linsys} does not need to be explicitly computed or stored, nor do we need to compute the matrix $P^T$. This approach can be applied with a small modification if $P$ is rank-deficient also: column pivoting can be used for the QR decomposition, and truncation of the singular values can be used for the SVD.

\subsubsection{Evaluation}

Once the coefficients $\vc$ and $\vd$ are computed, the interpolant \eqref{eq:hybrid} can be evaluated anywhere. Let $X_e = \{\vx^e_i \}_{i=1}^{N_e}$ be a set of \emph{evaluation points}. Then, the interpolant can be evaluated at $X_e$ as:
\begin{align}
\lf.s(\vx)\rt|_{X_e} = \begin{bmatrix} 
A_e & P_e 
\end{bmatrix}
\begin{bmatrix}
\vc\\
\vd
\end{bmatrix} = A_e \vc + P_e \vd,
\end{align}
where $A_e$ and $P_e$ are evaluations of the RBF and polynomial basis functions at $X_e$. In general, $A_e$ is a sparse matrix if Wendland RBFs are used, and its structure is determined by the smoothness of the Wendland RBF, the precise value of $\epsilon$ (and therefore $r$), and the number of evaluation points $N_e$.

\subsection{Hybrid approximation ($q < r$) }

\subsubsection{Interpolation}
If $r > q$ (or conversely $\epsilon$ is sufficiently small), the constraints \eqref{eq:interp_constraints} can no longer be represented by \eqref{eq:small_limit_linsys}. Instead, they now correspond to a similar block linear system with $I$ replaced by a sparse matrix $A$ with entries $A_{ij} = \phi\lf(\|\vx_i - \vx_j\|\rt)$:
\begin{align}
\begin{bmatrix}
A & P \\
P^T & 0
\end{bmatrix}
\begin{bmatrix}
\vc\\
\vd
\end{bmatrix}
=
\begin{bmatrix}
\vf\\
{\bf 0}
\end{bmatrix}.
\label{eq:general_linsys}
\end{align}
Once again, we may use block Gaussian elimination to rewrite this as
\begin{align}
P^T A^{-1}P \vd &= P^T A^{-1} \vf,\\
A\vc &= \vf - P\vd,
\end{align}
where now $A^{-1}$ appears in the equation for $\vd$, and finding $\vc$ requires the inversion of a sparse matrix $A$. The exact properties of this approximation depend on the value of $r$, the smoothness of the Wendland kernel, and the polynomial degree $\ell$. However, unlike in the polynomial limit $r<q$, both the RBFs and the polynomials participate in the approximation.

\subsubsection{Linear Algebra}
Unlike in the polynomial limit, it is not possible to solve the above pair of equations for $\vd$ and $\vc$ using a single $QR$ decomposition of the matrix $P$. This leaves actually inverting the matrix $P^T A^{-1} P$. Unfortunately, this is a non-trivial task. The condition number of $A$ can range from $O(1)$ in the case of a small support $r$ to arbitrarily large for large $r$. Further, we have observed that the matrix $P^T P$ is typically very ill-conditioned since it is formed by evaluations of polynomials at a possibly arbitrary set of collocation points. In addition, though the matrix $P^T A^{-1} P$ is symmetric positive definite in exact arithmetic (since $A^{-1}$ is symmetric positive-definite), we have observed that the product $P^T A^{-1} P$ loses symmetry due to rounding errors even when $A$ has a condition number of $O(1)$. Fortunately, we have found that the symmetry of $P^T A^{-1} P$ can be restored using standard numerical linear algebra. Since $A$ is symmetric positive-definite, it can be written in terms of its Cholesky decomposition as
\begin{align}
A = LL^T,
\end{align}
where $L$ is a lower-triangular matrix. If $A$ is sparse, it is also possible to maintain sparsity in $L$. Using this decomposition, we can rewrite the Schur complement system as:
\begin{align}
P^T \lf(LL^T\rt)^{-1} P \vd &= P^T \lf(LL^T\rt)^{-1} \vf,\\
\implies P^T L^{-T} \underbrace{L^{-1} P}_{B} \vd &= P^T L^{-T} \underbrace{L^{-1} \vf}_{\vg},\\
\implies B^T B \vd &= B^T \vg,
\end{align}
where $B =L^{-1} P$. This is in fact a system of normal equations for $\vd$. To solve this, we can clearly decompose B as $B = QR$, and compute the polynomial coefficients as $\vd = R^{-1} Q^T \vg$. Of course, the cost also includes inverting the sparse lower-triangular matrix $L$. Nevertheless, this approach ensures that $P^T A^{-1} P$ is symmetric while also transforming the problem into one solvable by the $QR$ decomposition. Finally, the RBF coefficients $\vc$ can be computed as
\begin{align}
\vc = L^{-T}L^{-1}\lf(\vf - P \vd\rt)
\end{align}


%\section{Unified interpolation on manifolds}
%\label{sec:man_uni}
%
%One of the strengths of RBF interpolation is that it is applicable to interpolation of data sampled on a submanifold $\mathbb{M} \subset \mathbb{R}^d$ using pure Euclidean distances measured in $\mathbb{R}^d$. This feature has been used to develop spectral methods~\cite{FuselierWright2010, FuselierWright2014,FlyerWright09}. However, the inclusion of polynomial terms presents difficulties as the matrix $P$ may not have full rank if the manifold is algebraic. In this scenario, the linear algebra techniques presented above would fail even in the diagonal limit. We now present a technique based on least orthogonal interpolation (LOI) to overcome this. While LOI was used in the generation of RBF-based finite difference formulas (the RBF-LOI method)~\cite{SNKJCP2018,SWNJCP2020}, this is its first application to \emph{global} interpolation with RBFs and polynomials.
%
%\subsection{Least Orthogonal Interpolation (LOI)}
%\label{sec:loi}
%
%\subsection{Global RBF-LOI}
%\label{sec:rbfloi}
